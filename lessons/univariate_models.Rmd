---
title: "univariate models"
output: html_document
---

The goals of this lesson are to introduce univariate modeling using simple and
multivariate Ordinary Least Squares (OLS) regression, and to gain exposure to
the concept and methods of model comparison.

The content from this lesson is modified in part from lessons developed by
Jack Weiss and Jason Fridley. The original lessons can be found at the following links:

* <http://www.unc.edu/courses/2010fall/ecol/563/001/docs/lectures/lecture1.htm>
* <http://plantecology.syr.edu/fridley/bio793/lm.html>

## Lesson Outline
* <a href="#phil">Modeling philosophy</a>
* <a href="#sim">Simulate data</a>
* <a href="#mult">Multiple regression</a>
* <a href="#inter">Interaction effects</a>
* <a href="#comp">Model comparisons</a>

## <a href="#phil" name="phil">#</a> Modeling Philosophy
One of the simplest and most important ideas when constructing models can be 
stated succinctly:

> "All models are wrong but some models are useful" G.E. Box 

As scientists we have to be on guard that we are not too attached to our
models. Additionally it is often difficult to decide when a model is useful
or not because there are three general reasons for building a model:

* Hypothesis testing
* Exploring data
* Predicting data

These different goals of model building require different approaches and modes
of model evaluation. 
Hypothesis testing is typically geared towards testing a small handful of carefully
crafted ***A PRIORI*** hypotheses. 
In this context they model is typically judged useful if it is statistically 
significant.  
However, many times though the investigator does not have a clear *_a priori_*
hypothesis [(see post at Small Pond Science)](http://smallpondscience.com/2013/06/04/pretending-you-planned-to-test-that-hypothesis-the-whole-time/) 
and is instead examining if a large number of possibly relevant variables are
correlated with a response of interest. 
The goal of this kind of exploratory analysis is to generate hypotheses that can
be more carefully tested with **other datasets** and not with the dataset used
to generate them. 
Unfortunately very frequently and quite misleadingly the correlations that
are found to be strong in the exploratory analysis are then presented in a
hypo-deductive framework as if they were *_a prori_* [(see post on Dynamic Ecology)](https://dynamicecology.wordpress.com/2013/10/16/in-praise-of-exploratory-statistics/).
When using models to make predictions we are typically less concerned about the 
exact variables that are in the model and more concerned that we can predict 
observations not included in the building of the model (i.e., cross-validate
the model).

There are many reasons that p-values and statistical significance are abused in
science. For example, it can be very tempting to report statistics of
significance in many analyses in which you did not have clear *_a prori_* 
hypotheses because often times R will report such statistics without prompting
from the user (e.g., `summary(my_ols_model)`). 
Additionally there is a stigma in many fields of science against exploratory 
analyses in favor of hypothesis testing which pushes some researchers to 
re-frame their analyses as if they are confirmatory rather than exploratory. 
And of course there is pressure during peer-review to only report on 
statistics that are significant. 

You might be wondering why this is a big deal. The reason is that you will inevitably
get good fitting models (high R^2) and statistically significant results (p < 0.05)
if you keep adding variables to a model even if those variables by definition are
independent of the response variable.  [(Freedman 1983)](http://amstat.tandfonline.com/doi/abs/10.1080/00031305.1983.10482729#.Ul17gVAkJPQ).


## <a href="#sim" name="sim">#</a> Simulate Hypothetical Data
R is an excellent environment for learning about how models work in part because
of the ease to generate data with known properties. This provides us the ability
to not only check that a model is performing as expected but also helps to 
indicate strengths and weaknesses of various model fitting and effect size 
strength measures. 

```{r}
#generate data for example
set.seed(10) # this is done so that the same results are generated each time
x1 = runif(90)
x2 = rbinom(90, 10, .5)
x3 = rgamma(90, .1, .1)

#organize predictors in data frame
sim_data = data.frame(x1, x2, x3)
#create noise b/c there is always error in real life
epsilon = rnorm(90, 0, 3)
#generate response: additive model plus noise, intercept=0
sim_data$y = 2*x1 + x2 + 3*x3 + epsilon
```

Above we have defined response variable `y` as a linear function of the three
simulated independent variables (the `x` variables). Epsilon refers to the 
error in the model and in line with OLS regression assumptions we have made 
this is a normally distributed variable centered at zero.

Now that we have simulated our data let's examine how we build OLS models in 
R and plot the result.

```{r}
#First we will demonstrate the simplest possible model 
#the intercept only model
mod = lm(sim_data$y ~ 1)
mod 
summary(mod)
#Note that the estimate for the intercept is equilalent to the mean of y
mean(sim_data$y)
```

We can easily accommodate more complex models but simply including them in our 
call to `lm`. By default models will always include an intercept.

```{r}
#simple linear regression with x1 as predictor
mod1 = lm(y ~ x1, data=sim_data)
#plot regression line and mean line
plot(y ~ x1, data=sim_data)
abline(h=mean(sim_data$y), col='pink', lwd=3)
abline(mod1, lty=2)
#simple linear regression with x3 as a predictor
mod3 = lm(y ~ x3, data=sim_data)
#graph regression line and mean line
plot(y ~ x3, data=sim_data)
abline(mod3)
abline(h=mean(sim_data$y), col='pink', lwd=2)
legend('topleft', c('OLS fit', 'mean'), col=c('black', 'pink'), lty=1, lwd=c(1,2))
```

Now that we've build our two models let's examine the model fits and the 
statistical significance of the explanatory variables. 

```{r}
summary(mod1)
```

R provides several useful pieces of information above. The coefficients table

```
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   9.6623     2.4493   3.945  0.00016 ***
x1           -0.5975     4.8060  -0.124  0.90134    
```

which provides the estimate, standard error, t-statistic, and p-value for:
 
* \(\hat{\beta_0}\) the y-intercept - 1st row of the table
* \(\hat{\beta_1}\) the slope of the regression line - 2nd row of the table

The p-values can be used to assess statistical significance, and
the t-statistics provide a measure of effect size.

In addition to the coefficient table several statistics for 
the entire model are also provided

```
Residual standard error: 11.86 on 88 degrees of freedom
Multiple R-squared:  0.0001756,	Adjusted R-squared:  -0.01119 
F-statistic: 0.01546 on 1 and 88 DF,  p-value: 0.9013
```

The multiple R-squared and adjusted R-squared provide estimates of variance 
explained. 
The later statistic adjusts for the number of variables included in the model.
The F-statistic is a ratio of the mean sum of squares for the model to the 
sum of squares of the residuals (i.e., the ratio of explained variance to 
unexplained variance). The p-value associated with the F-statistic provides
a means of examining the statistics significance of the entire model. 

We noticed that there was a large outlier in the previous plot. Let's remove it
to see how this changes the fitted regression model. 

```{r}
sim_data_sub = sim_data[sim_data$y < 25,]
#verify that one observation was removed
dim(sim_data)
dim(sim_data_sub)
#refit model to reduced data
mod3_sub = lm(y ~ x3, data=sim_data_sub)
summary(mod3)
summary(mod3_sub)
```

So it appears that \(R^2\) is highly sensitive to outliers but the \(\beta\) 
coefficients are more robust. 

### Excercise: 
Create a single plot that displays the model of y given x3 before and after the
outliers were removed. How much to they visually differ from one another.
Examine the arguments to `abline()` including `lty` and `lwd`.

## <a href="#mult" name="mult">#</a> Multiple regression 

```{r}
mod_main = lm(y ~ x1 + x2 + x3, data=sim_data)
summary(mod_main)
```

## <a href="#inter" name="inter">#</a> Interaction effects

```{r}
# you can build the model by writing out every interaction
lm(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + x1*x2*x3, data=sim_data)
# or you can simply update the main effects model
mod_full = update(mod_main, ~ . + x1*x2*x3)
summary(mod_full)
```

## <a href="#comp" name="comp">#</a> Model comparisons

```{r}
# we can carry out a nested model comparison
anova(mod_main, mod_full)

# examine the AIC scores of the two models, smaller number is better
AIC(mod_full)
AIC(mod_main)

# run a stepwise regerssion analysis on the full model.
library(MASS)
stepAIC(mod_full)
```

