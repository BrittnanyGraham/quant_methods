---
title: "univariate models"
output: html_document
---

The goals of this lesson are to introduce univariate modeling using simple and
multivariate Ordinary Least Squares (OLS) regression, and to gain exposure to
the concept and methods of model comparison.

The content from this lesson is modified in part from lessons developed by Jack Weiss and Jason Fridley. The original lessons can be found at the following links:

* http://www.unc.edu/courses/2010fall/ecol/563/001/docs/lectures/lecture1.htm
* http://plantecology.syr.edu/fridley/bio793/lm.html

## Lesson Outline
* Modeling philosophy
* Simulate Data
* Model building
* 

## Modeling Philosophy
One of the simplest and most important ideas when constructing models can be 
stated succinctly:

> "All models are wrong but some models are useful" G.E. Box 

As scientists we have to be on guard that we are not too attached to our
models. Additionally it is often difficult to decide when a model is useful
or not because there are three general reasons for building a model:

* Hypothesis testing
* Explaining or exploring data
* Predicting data

These different goals of model building require different approaches and modes
of evaluation of model efficitiveness. A full discussion of the differences in 
the philosophies and approaches is beyond the scope of this lesson. 

## Simulate Hypothetical Data
R is an excellent environment for learning about how models work in part because
of the ease to generate data with known properties. This provides us the ability
to not only check that a model is performing as expected but also helps to 
indicate strengths and weaknesses of various model fitting and effect size 
strength measures. 

```{r}
#generate data for example
set.seed(10)
x1 = runif(90)
x2 = rbinom(90, 10, .5)
x3 = rgamma(90, .1, .1)

#organize predictors in data frame
sim_data = data.frame(x1, x2, x3)
#create noise b/c there is always error in real life
epsilon = rnorm(90, 0, 3)
#generate response: additive model plus noise, intercept=0
sim_data$y = 2*x1 + x2 + 3*x3 + epsilon
```

Above we have defined response variable "y" as a linear function of the three
simulated independent variables (the x variables). Epsilon refers to the 
error in the model and in line with OLS regression assumptions we have made 
this is a normally distributed variable centered at zero.

Now that we have simulated our data let's examine how we build OLS models in 
R and plot the result.

```{r}
#First we will demonstrate the simplest possible model 
#the intercept only model
mod = lm(sim_data$y ~ 1)
mod 
summary(mod)
#Note that the estimate for the intercept is equilalent to the mean of y
mean(sim_data$y)
```

We can easily accomodate more complex models but simply including them in our 
call to lm. Be default models will always include an intercept.

```{r}
#simple linear regression with x1 as predictor
mod1 = lm(y ~ x1, data=sim_data)
#plot regression line and mean line
plot(y ~ x1, data=sim_data)
abline(h=mean(sim_data$y), col='pink', lwd=3)
abline(mod1, lty=2)
#simple linear regression with x3 as a predictor
mod3 = lm(y ~ x3, data=sim_data)
#graph regression line and mean line
plot(y ~ x3, data=sim_data)
abline(mod3)
abline(h=mean(sim_data$y), col='pink', lwd=2)
legend('topleft', c('OLS fit', 'mean'), col=c('black', 'pink'), lty=1, lwd=c(1,2))
```

Now that we've build our two models let's examine the model fits and the 
statistical significance of the explanatory variables. 

```{r}
summary(mod1)
```

R provides several useful pieces of information above. The coefficients table

```
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   9.6623     2.4493   3.945  0.00016 ***
x1           -0.5975     4.8060  -0.124  0.90134    
```

which provides the estimate, standard error, t-statistic, and p-value for:
 
* \(\hat{\beta_0}\) the y-intercept - 1st row of the table
* \(\hat{\beta_1}\) the slope of the regression line - 2nd row of the table

We noticed that there was a large outlier in the previous plot. Let's remove it
to see how this changes the fitted regression model. 

```{r}
sim_data_sub = sim_data[sim_data$y < 25,]
#verify that one observation was removed
dim(sim_data)
dim(sim_data_sub)
#refit model to reduced data
mod3_sub = lm(y ~ x3, data=sim_data_sub)
summary(mod3)
summary(mod3_sub)
```

So it appears that \(R^2\) is highly sensative to outliers but the \(beta\) 
coefficients are more robust. 

### Excercise: 
Create a single plot that displays the model of y given x3 before and after the
outliers were removed. How much to they visually differ from one another.
Examine the arguments to `abline()` including `lty` and `lwd`.

## Multiple regression 

```{r}
mod_main = lm(y ~ x1 + x2 + x3, data=sim_data)
summary(mod_main)
```

## Interaction effects

```{r}
# you can build the model by writing out every interaction
lm(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + x1*x2*x3, data=sim_data)
# or you can simply update the main effects model
mod_full = update(mod_main, ~ . + x1*x2*x3)
summary(mod_full)
```

## Model comparisons

```{r}
# we can carry out a nested model comparison
anova(mod_main, mod_full)

# examine the AIC scores of the two models, smaller number is better
AIC(mod_full)
AIC(mod_main)

# run a stepwise regerssion analysis on the full model.
library(MASS)
stepAIC(mod_full)
```

