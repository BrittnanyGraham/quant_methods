---
title: "univariate models"
output: html_document
---

<!--clickable code reveal: 
    http://stackoverflow.com/questions/14127321/how-to-hide-code-in-rmarkdown-with-option-to-see-it
-->
<style>
div.hidecode + pre {display: none}
</style>
<script>
doclick = function(e){
    e.nextSibling.nextSibling.style.display="block";
}
</script>

The goals of this lesson are to introduce univariate modeling using simple and
multivariate Ordinary Least Squares (OLS) regression, and to gain exposure to
the concept and methods of model comparison.

The content from this lesson is modified in part from lessons developed by
Jack Weiss and Jason Fridley. The original lessons can be found at the following links:

* <http://www.unc.edu/courses/2010fall/ecol/563/001/docs/lectures/lecture1.htm>
* <http://plantecology.syr.edu/fridley/bio793/lm.html>

## Readings
* Chapters 5, 9-10 in *The R book* (1st ed) by Crawley 
* Chapters 6 in *MASS* (4th ed) by Venables and Ripley

## Lesson Outline
* <a href="#phil">Modeling philosophy</a>
* <a href="#sim">Simulate data</a>
* <a href="#mult">Multiple regression</a>
* <a href="#stan">Standardized \(\beta\) coefficients</a>
* <a href="#inter">Interaction effects</a>
* <a href="#comp">Model comparisons</a>

## <a href="#phil" name="phil">#</a> Modeling Philosophy
One of the simplest and most important ideas when constructing models can be 
stated succinctly:

> "All models are wrong but some models are useful" G.E. Box 

As scientists we have to be on guard that we are not too attached to our
models. Additionally it is often difficult to decide when a model is useful
or not because there are three general reasons for building a model:

* Hypothesis testing
* Exploring data
* Predicting data

These different goals of model building require different approaches and modes
of model evaluation. 
Hypothesis testing is typically geared towards testing a small handful of carefully
crafted ***A PRIORI*** hypotheses. 
In this context they model is typically judged useful if it is statistically 
significant.  
However, many times though the investigator does not have a clear *_a priori_*
hypothesis [(see post at Small Pond Science)](http://smallpondscience.com/2013/06/04/pretending-you-planned-to-test-that-hypothesis-the-whole-time/) 
and is instead examining if a large number of possibly relevant variables are
correlated with a response of interest. 
The goal of this kind of exploratory analysis is to generate hypotheses that can
be more carefully tested with **other datasets** and not with the dataset used
to generate them. 
Unfortunately very frequently and quite misleadingly the correlations that
are found to be strong in the exploratory analysis are then presented in a
hypo-deductive framework as if they were *_a prori_* [(see post on Dynamic Ecology)](https://dynamicecology.wordpress.com/2013/10/16/in-praise-of-exploratory-statistics/).
When using models to make predictions we are typically less concerned about the 
exact variables that are in the model and more concerned that we can predict 
observations not included in the building of the model (i.e., cross-validate
the model).

There are many reasons that p-values and statistical significance are abused in
science. For example, it can be very tempting to report statistics of
significance in many analyses in which you did not have clear *_a prori_* 
hypotheses because often times R will report such statistics without prompting
from the user (e.g., `summary(my_ols_model)`). 
Additionally there is a stigma in many fields of science against exploratory 
analyses in favor of hypothesis testing which pushes some researchers to 
re-frame their analyses as if they are confirmatory rather than exploratory. 
And of course there is pressure during peer-review to only report on 
statistics that are significant. 

You might be wondering why this is a big deal. The reason is that you will inevitably
get good fitting models (high R^2) and statistically significant results (p < 0.05)
if you keep adding variables to a model even if those variables by definition are
independent of the response variable.  [(Freedman 1983)](http://amstat.tandfonline.com/doi/abs/10.1080/00031305.1983.10482729#.Ul17gVAkJPQ).

### The Principle of Parsimony (Occam’s Razor)

The principle of parsimony can be stated succinctly as: 

> the correct explanation is the simplest explanation.

In the context of model building, all else being equal the better model is the 
model with the following properties (Crawley, 2007):

* a model with n−1 parameters to a model with n parameters;
* a model with k−1 explanatory variables to a model with k explanatory variables;
* a linear model to a model which is curved;
* a model without a hump to a model with a hump;
* a model without interactions to a model containing interactions between factors.

Thus model simplification is an important part of finding the most useful model.
A simpler model that explains similar levels of deviance in a response should be
considered a better model. 

Crawley (2007) suggests the following steps for backward model simplification:

![model_simpl](../figures/crawley_2007_table9_2_model_simplification.png)

These steps are reasonable if one is carrying out an exploratory data analysis; 
however, I do not recommend this approach when formal hypothesis testing is the 
goal of the analysis due to Freedman's pardox as explained above. Additionally,
it may be worth pointing out that model building can procede from simple to more
complex models as well evaulating with each additional term if it has significantly
increased explanatory power. 

## <a href="#sim" name="sim">#</a> Simulate Hypothetical Data
R is an excellent environment for learning about how models work in part because
of the ease to generate data with known properties. This provides us the ability
to not only check that a model is performing as expected but also helps to 
indicate strengths and weaknesses of various model fitting and effect size 
strength measures. 

```{r}
#generate data for example
set.seed(10) # this is done so that the same results are generated each time
x1 = runif(90)
x2 = rbinom(90, 10, .5)
x3 = rgamma(90, .1, .1)

#organize predictors in data frame
sim_data = data.frame(x1, x2, x3)
#create noise b/c there is always error in real life
epsilon = rnorm(90, 0, 3)
#generate response: additive model plus noise, intercept=0
sim_data$y = 2*x1 + x2 + 3*x3 + epsilon
```

Above we have defined response variable `y` as a linear function of the three
simulated independent variables (the `x` variables). Epsilon refers to the 
error in the model and in line with OLS regression assumptions we have made 
this is a normally distributed variable centered at zero.

Now that we have simulated our data let's examine how we build OLS models in 
R and plot the result.

```{r}
#First we will demonstrate the simplest possible model 
#the intercept only model
mod = lm(sim_data$y ~ 1)
mod 
summary(mod)
#Note that the estimate for the intercept is equilalent to the mean of y
mean(sim_data$y)
```

We can easily accommodate more complex models but simply including them in our 
call to `lm`. By default models will always include an intercept.

```{r}
#simple linear regression with x1 as predictor
mod1 = lm(y ~ x1, data=sim_data)
#plot regression line and mean line
plot(y ~ x1, data=sim_data)
abline(h=mean(sim_data$y), col='pink', lwd=3)
abline(mod1, lty=2)
#simple linear regression with x3 as a predictor
mod3 = lm(y ~ x3, data=sim_data)
#graph regression line and mean line
plot(y ~ x3, data=sim_data)
abline(mod3)
abline(h=mean(sim_data$y), col='pink', lwd=2)
legend('topleft', c('OLS fit', 'mean'), col=c('black', 'pink'), lty=1, lwd=c(1,2))
```

Now that we've build our two models let's examine the model fits and the 
statistical significance of the explanatory variables. 

```{r}
summary(mod1)
```

R provides several useful pieces of information above. The coefficients table

```
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   9.6623     2.4493   3.945  0.00016 ***
x1           -0.5975     4.8060  -0.124  0.90134    
```

which provides the estimate, standard error, t-statistic, and p-value for:
 
* \(\hat{\beta_0}\) the y-intercept - 1st row of the table
* \(\hat{\beta_1}\) the slope of the regression line - 2nd row of the table

The p-values can be used to assess statistical significance, and
the t-statistics provide a measure of effect size.

In addition to the coefficient table several statistics for 
the entire model are also provided

```
Residual standard error: 11.86 on 88 degrees of freedom
Multiple R-squared:  0.0001756,	Adjusted R-squared:  -0.01119 
F-statistic: 0.01546 on 1 and 88 DF,  p-value: 0.9013
```

The multiple R-squared and adjusted R-squared provide estimates of variance 
explained. 
The later statistic adjusts for the number of variables included in the model.
The F-statistic is a ratio of the mean sum of squares for the model to the 
sum of squares of the residuals (i.e., the ratio of explained variance to 
unexplained variance). The p-value associated with the F-statistic provides
a means of examining the statistics significance of the entire model. 

We noticed that there was a large outlier in the previous plot. Let's run model 
diagonstics to see if we should consider dropping that variable. 

```{r}
par(mfrow=c(2,2))
plot(mod1)
par(mfrow=c(1,1))
```

From the diagonsitic plots you can see observations 26, 85, and 88 are consistently
identified as having abnormally large residual values (Residuals vs Fitted plot),
they cause the residuals to diverge from an expectation under normality (Normal Q-Q plot),
and lastly they exert too much leverage (i.e., control on the slope of the 
regression, Residuals vs Leverage plot). 

Here is a case where if this was an actual analysis we would check to make sure 
these values are not mistakes. Let's drop these points and examine the changes
in the model statistics. 

```{r}
sim_data_sub = sim_data[-c(26, 85, 88), ]
#verify that one observation was removed
dim(sim_data)
dim(sim_data_sub)
#refit model to reduced data
mod3_sub = lm(y ~ x3, data=sim_data_sub)
summary(mod3)
summary(mod3_sub)
```

So it appears that \(R^2\) is highly sensitive to outliers but the \(\beta\) 
coefficients are more robust. 

### Excercise: 
Create a single plot that displays the model of y given x3 before and after the
outliers were removed. How much to they visually differ from one another.
Examine the arguments to `abline()` including `lty` and `lwd`.

<div class="hidecode" onclick="doclick(this);">[Show Code]</div>
```{r}
plot(y ~ x3, data=sim_data)
points(y ~ x3, data=sim_data_sub, col='dodgerblue', pch=19)
abline(mod3)
abline(mod3_sub, col='dodgerblue', lwd=2)
legend('topleft', c('fit with all data', 'fit w/o outliers'), 
       col=c('black', 'dodgerblue'), pch=c(1, 19), lty=1, 
       lwd=c(1,2), bty='n')
```

## <a href="#mult" name="mult">#</a> Multiple regression 

So far we have only examined models with a single variable but by design we
know that y is influenced by three variables. Let's include all the relevant
explanatory variables in one model now. 

```{r}
mod_main = lm(y ~ x1 + x2 + x3, data=sim_data)
summary(mod_main)
coefficients(mod_main)
```

Notice that in the output above the coefficient estimates are close to 
what we set them at when we created the variable `y`. 


## <a href="#stan" name="stan">#</a> Standardized \(\beta\) coefficients
A standardized \(\beta\) or regression coefficient is simply the 
\(\beta\) estimate from a regression on standardized variables. The typical way
of standardizing a variable is to subtract its mean and divide it by its standard
deviation. This transformation results in a variable a mean of zero and a
standard deviation of one. There are other ways to standardize a variable but
this is by far the most common.

One reason for standardizing variables is that you can interpret the \(\beta\) estimates as partial correlation coefficients. In other words now that the
variables are standardized you can compare how correlated they are to the 
response variable using their regression coefficients. Below is a demo of this.

```{r}
## We will use this function to plot the data and correlations 
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor=3, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) 
        cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor)
}
```

Before standardizing variables it is worthwhile to highlight that the 
relationship between correlation and regression statistics. Specifically, 
the t-statistic from a simple correlation coefficient is exactly what is
reported for the \(\beta_1\)  coefficient in a regression model.

```{r}
cor.test(sim_data$y, sim_data$x1)$statistic
summary(lm(y ~ x1, data=sim_data))$coef
```

The \(\beta\) coefficient reported by the regression is not equal to the
correlation coefficient though because the \(\beta\) is in the units of the 
\(x_1\) variable (i.e., it has not been standardized). Now let's use the function 
`scale()` to standardize the independent and dependent variables. 

```{r}
sim_data_std = data.frame(scale(sim_data))

mod = lm(y  ~ x1 + x2 + x3, data=sim_data)
mod_std = lm(y  ~ x1 + x2 + x3, data=sim_data_std)
round(summary(mod)$coef, 3)
round(summary(mod_std)$coef, 3)
cor(sim_data$y, sim_data$x1)
cor(sim_data$y, sim_data$x2)
cor(sim_data$y, sim_data$x3)

```

Notice that above the t-statistics and consequently the p-values between `mod`
and `mod_std` don't change (with the exception of the intercept term which is
always 0 in a regression of standardized variables). This is because the 
t-statistic is a pivotal statistic meaning that its value doesn't depend on the
scale of the difference. 

Additionally notice that the individual correlation coefficients are very 
similar to the \(\beta\) estimates in `mod_std`. Why are these not exactly the same?
Here's a hint - what would happen if their was strong multicollinarity between
the explanatory variables?

Let's plot the variables against one another and also display their individual
Pearson correlation coefficients to get a visual perspective on the problem

```{r}
pairs(sim_data, lower.panel = panel.smooth, upper.panel = panel.cor)
```

## <a href="#inter" name="inter">#</a> Interaction effects

```{r}
# you can build the model by writing out every interaction
lm(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + x1*x2*x3, data=sim_data)
# or you can simply update the main effects model
mod_full = update(mod_main, ~ . + x1*x2*x3)
summary(mod_full)
```

## <a href="#comp" name="comp">#</a> Model comparisons

```{r}
# we can carry out a nested model comparison
anova(mod_main, mod_full)

# examine the AIC scores of the two models, smaller number is better
AIC(mod_full)
AIC(mod_main)

# run a stepwise regerssion analysis on the full model.
library(MASS)
stepAIC(mod_full)
```

